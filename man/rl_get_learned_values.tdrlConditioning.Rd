% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/func-rl_get_learned_values.R
\name{rl_get_learned_values.tdrlConditioning}
\alias{rl_get_learned_values.tdrlConditioning}
\title{Get 'tdrlConditioning' Agent's Learned Value Data}
\usage{
\method{rl_get_learned_values}{tdrlConditioning}(agent, add_trial_zero = TRUE, trial_zero_value = 0, ...)
}
\arguments{
\item{agent}{An object of class "rlAgent" created with
\code{\link{rl_new_agent}}.}

\item{add_trial_zero}{(Logical) \code{TRUE} by default and trial zero will be
appended to the learned value data frame with values from
\code{trial_zero_value}. \code{FALSE} and output will begin at trial one.}

\item{trial_zero_value}{(Numeric) Either a single value (default is 0) or a
vector of values to append for trial 0.}

\item{...}{Additional arguments passed to or from other methods.}
}
\value{
A dataframe with the agent's simulated reward learned values
('value') for each episode across trials.
}
\description{
Convert the 'tdrl' agent's simulated reward learned values from a
matrix where each row is an episode and each column is a trial to a
dataframe with columns 'trial', 'episode', and 'value'.
}
