% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/R6_tdrl_conditioning.R
\name{agent_tdrl_conditioning}
\alias{agent_tdrl_conditioning}
\title{R6 Class representing a new TDRL agent for Conditioning Simulations}
\description{
R6 Class representing a new TDRL agent for Conditioning Simulations

R6 Class representing a new TDRL agent for Conditioning Simulations
}
\details{
Called by \code{\link{rl_new_agent}}.
}
\keyword{internal}
\section{Public fields}{
\if{html}{\out{<div class="r6-fields">}}
\describe{
\item{\code{model_id}}{(character) A model identifier referencing the
reinforcement learning paradigm to perform the simulation.}

\item{\code{num_trials}}{(numeric) The number of trials to simulate.}

\item{\code{num_episodes}}{(numeric) The number of episodes per trial.}

\item{\code{gamma}}{(numeric) The temporal discounting factor of the RL agent}

\item{\code{alpha}}{(numeric) The learning rate of the RL agent}

\item{\code{num_cues}}{(numeric) The number of potential cues interacting with the RL agent.}

\item{\code{cue_structure}}{A three-dimensional array tacking what cues are
present in each episode across all simulated trials}

\item{\code{estimated_values}}{A three dimensional array tracking the estimated
values of each episode across all trials}

\item{\code{num_reinforcements}}{(numeric) The number of potential reinforcements
experienced by the RL agent.}

\item{\code{reinforcements}}{A matrix tracking experienced rewards in each episode
across all trials}

\item{\code{prediction_errors}}{A three dimensional array tracking the
prediction errors associated with each arm across all episodes and
trials}

\item{\code{simulation_code}}{The code for simulating the kArmedBandit}
}
\if{html}{\out{</div>}}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-new}{\code{agent_tdrl_conditioning$new()}}
\item \href{#method-print}{\code{agent_tdrl_conditioning$print()}}
\item \href{#method-set_reinforcements}{\code{agent_tdrl_conditioning$set_reinforcements()}}
\item \href{#method-set_cues}{\code{agent_tdrl_conditioning$set_cues()}}
\item \href{#method-simulate_agent}{\code{agent_tdrl_conditioning$simulate_agent()}}
\item \href{#method-get_learned_values}{\code{agent_tdrl_conditioning$get_learned_values()}}
\item \href{#method-get_pe_data}{\code{agent_tdrl_conditioning$get_pe_data()}}
\item \href{#method-get_simulation_code}{\code{agent_tdrl_conditioning$get_simulation_code()}}
\item \href{#method-clone}{\code{agent_tdrl_conditioning$clone()}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-new"></a>}}
\if{latex}{\out{\hypertarget{method-new}{}}}
\subsection{Method \code{new()}}{
Create a new \code{tdrlConditioning} object
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{agent_tdrl_conditioning$new(model_id, num_trials, num_episodes, gamma, alpha)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{model_id}}{model_id (character) A model identifier referencing the
reinforcement learning paradigm to perform the simulation.}

\item{\code{num_trials}}{(numeric) The number of trials to simulate.}

\item{\code{num_episodes}}{(numeric) The number of episodes per trial.}

\item{\code{gamma}}{(numeric) The temporal discounting factor of the RL agent}

\item{\code{alpha}}{(numeric) The learning rate of the RL agent}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-print"></a>}}
\if{latex}{\out{\hypertarget{method-print}{}}}
\subsection{Method \code{print()}}{
Printing method for object of class 'rlAgent'.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{agent_tdrl_conditioning$print(...)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{...}}{NA; printing function}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-set_reinforcements"></a>}}
\if{latex}{\out{\hypertarget{method-set_reinforcements}{}}}
\subsection{Method \code{set_reinforcements()}}{
Define the onset episode and offset episode of rewards for
each trial
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{agent_tdrl_conditioning$set_reinforcements(
  reinforcement_input,
  keep_reward_structure = FALSE
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{reinforcement_input}}{A list of reinforcements where each element
contains a data frame with columns 'onset', 'offset', 'magnitude', and
'trial' describing, respectively, the episode number a reward
presentation begins; the episode number the reward presentation ends;
the magnitude of the reward; the trials the rewards occur on.}

\item{\code{keep_reward_structure}}{(Logical) \code{FALSE} (default) and any existing
reward structure will be replaced when called. \code{TRUE} and the reward
structure will be modified but will not remove previously defined
rewards.}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-set_cues"></a>}}
\if{latex}{\out{\hypertarget{method-set_cues}{}}}
\subsection{Method \code{set_cues()}}{
Define the onset episode and offset episode of cues for each
trial.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{agent_tdrl_conditioning$set_cues(cue_input, keep_cue_structure = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{cue_input}}{A list of cues where each element contains a data frame
with columns 'onset', 'offset', 'magnitude', and 'trial' describing,
respectively, the episode number a cue presentation begins; the episode
number the cue presentation ends; the magnitude (saliency of a cue) of
the cue; the trials the cues occur.}

\item{\code{keep_cue_structure}}{(Logical) \code{FALSE} (default) and any existing
cue structure will be replaced when called. \code{TRUE} and the cue
structure will be modified but will not remove previously defined cues.}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-simulate_agent"></a>}}
\if{latex}{\out{\hypertarget{method-simulate_agent}{}}}
\subsection{Method \code{simulate_agent()}}{
Simulate the RL Agent
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{agent_tdrl_conditioning$simulate_agent()}\if{html}{\out{</div>}}
}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-get_learned_values"></a>}}
\if{latex}{\out{\hypertarget{method-get_learned_values}{}}}
\subsection{Method \code{get_learned_values()}}{
Convert the agent's estimated values array where, for each
cue, there is a matrix where each row is an episode and each column
is a trial that contains the estimated value, to a dataframe with columns
'trial', 'episode', 'cue', and 'value'.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{agent_tdrl_conditioning$get_learned_values(
  add_trial_zero = TRUE,
  trial_zero_value = 0
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{add_trial_zero}}{(Logical) \code{TRUE} by default and trial zero will be
appended to the prediction error data frame with values from
\code{trial_zero_value}. \code{FALSE} and output will begin at trial one.}

\item{\code{trial_zero_value}}{(Numeric) Either a single value (default is 0) or
a vector of values with length \code{num_arms} times \code{num_episodes} to
append for trial 0.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
A dataframe with the agent's simulated learned values ('value')
a given cue corresponds to for each episode across all trials.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-get_pe_data"></a>}}
\if{latex}{\out{\hypertarget{method-get_pe_data}{}}}
\subsection{Method \code{get_pe_data()}}{
Convert the agent's simulated reward prediction errors from
a matrix where each row is an episode and each column is a trial to a
dataframe with columns 'trial', 'episode', and 'value'.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{agent_tdrl_conditioning$get_pe_data(
  add_trial_zero = TRUE,
  trial_zero_value = 0
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{add_trial_zero}}{(Logical) \code{TRUE} by default and trial zero will be
appended to the prediction error data frame with values from
\code{trial_zero_value}. \code{FALSE} and output will begin at trial one.}

\item{\code{trial_zero_value}}{(Numeric) Either a single value (default is 0) or
a vector of values to append for trial 0.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
A dataframe with the agent's simulated reward prediction errors
('value') for each episode across trials.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-get_simulation_code"></a>}}
\if{latex}{\out{\hypertarget{method-get_simulation_code}{}}}
\subsection{Method \code{get_simulation_code()}}{
Retrieve the code needed to simulate the agent.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{agent_tdrl_conditioning$get_simulation_code()}\if{html}{\out{</div>}}
}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-clone"></a>}}
\if{latex}{\out{\hypertarget{method-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{agent_tdrl_conditioning$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
