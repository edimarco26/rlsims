% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/func_rl_simulate_action.R
\name{rl_simulate_action.epsilonGreedy}
\alias{rl_simulate_action.epsilonGreedy}
\title{Simulate an Action with a 'Epsilon-Greedy' Choice Policy}
\usage{
\method{rl_simulate_action}{epsilonGreedy}(policy = "epsilonGreedy", values, epsilon, ...)
}
\arguments{
\item{policy}{Defines the action selection policy as "epsilonGreedy";
argument included in this method to support S3 Generics.}

\item{values}{A numeric vector containing the current value estimates of each
action.}

\item{epsilon}{A parameter (between zero and one) modulating the RL agent's
propensity to explore. That is, the higher the epsilon, the less
exploitative choices the RL agent will make.}

\item{...}{Additional arguments passed to or from other methods.}
}
\value{
A number representing which action will be taken.
}
\description{
This implementation of an 'epsilonGreedy' action selection
policy accepts a parameter \code{epsilon}, which describes an agent's propensity
to explore the action space. The higher the epsilon, the more likely the
agent is to select a random action; the lower epsilon, the more likely the
agent is to select the exploitative action (one with highest expected
value).
}
