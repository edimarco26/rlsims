% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/rl_agent.R
\name{rl_get_tidy_pe_data}
\alias{rl_get_tidy_pe_data}
\title{Get Tidied Reward Prediction Error Data}
\usage{
rl_get_tidy_pe_data(agent, add_trial_zero = TRUE, trial_zero_value = 0)
}
\arguments{
\item{agent}{An object of class "rlAgent" created with
\code{\link{rl_new_agent}}.}

\item{add_trial_zero}{(Logical) \code{TRUE} by default and trial zero will be
appended to the prediction error data frame with values from
\code{trial_zero_value}. \code{FALSE} and output will begin at trial one.}

\item{trial_zero_value}{(Numeric) Either a single value (default is 0) or a
vector of values to append for trial 0.}
}
\value{
A dataframe with the agent's simulated reward prediction errors
('value') for each episode across trials.
}
\description{
Convert the agent's simulated reward prediction errors from a
matrix where each row is an episode and each column is a trial to a
dataframe with columns 'trial', 'episode', and 'value'.
}
