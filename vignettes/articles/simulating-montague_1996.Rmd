---
title: "Case Study: Montague, Dayan, and Sejnowski (1996)"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{simulating-montague-1996}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
library(magrittr)
library(stats)
```

```{r setup}
library(rlsims)
```

In the mid 1990s, P. Read Montague, Peter Dayan, and Terry Sejnowski described how the activity of mesencephalic dopaminergic neurons is predicted by a temporal-difference reinforcement learning (TDRL) algorithm. In this vignette, we walk through how to simulate their work using `rlsims`.

We'll begin with \_\_\_\_.

## Figure 6

Figure 6 demonstrates how the TDRL algorithm performs in a simulation of a delayed spatial choice task with 500 trials and 100 episodes per trial.

### 6A

> An initial sensory cue is presented at episode 60 with a second sensory cue presented randomly between 9 and 11 episodes later. A reward is presented at episode 80, lasting for one episode. Learning rate is 0.05.

To construct a simulation agent with these constraints, we need to create a new agent with `rl_new_agent()`. We set the `model_type = "tdrl"`, set `num_trials = 500` and `num_episodes = 100`, as described above. The temporal discounting factor, $0 \leq \gamma \leq 1$, is assumed to be one, so we set `gamma = 1`. Similarly, the learning rate, $\alpha$, was defined as 0.05, so we set `alpha = 0.05`.

```{r agent-6a}

agent_6a <- rl_new_agent(model_type = "tdrlConditioning",
                         model_id = "Figure 6A",
                         num_trials = 500,
                         num_episodes = 100,
                         gamma = 1,
                         alpha = 0.05)

```

Agent objects have special printing properties, describing the necessary information for an agent:

```{r print-agent-6a}
agent_6a
```

We see that the reward structure and cue structures are not set. To do that, we can use the functions `rl_set_reinforcements()` and `rl_set_cues()`. The reward should occur at episode 80, which we specify with `reward_onset`; since it only lasted for one episode, we set the `reward_offset` equal to 80, too. In this example, the reward magnitude was 1. However, we can change that if desired with the `reward_magnitude` argument.

```{r set_reinforcements_6a}
agent_6a %>%
  rl_set_reinforcements(
    list(
      data.frame(
        onset = 80,
        offset = 80,
        magnitude = 1,
        trial = 1:500
      )
    )
  ) %>%
  print()
```

By printing it, we can see that the reward structure has been set. To set the cue structure, we must construct a list of data frames, as written in the documentation for `rl_set_cues()`. Specifically, we need a data frame for each cue (in this case two) with columns 'onset', 'offset', 'magnitude', and 'trial'. the onset column should be a number defining in what episode the cue first occurs; the offset column should be a number defining in what episode the cue ends; the magnitude column should be 1 if the cue is present and 0 otherwise, and the trial column should be a numeric vector representing each trial to be modified. This explicit definition allows the cues to be easily changed across trials (see examples). We can construct the cue list for replicating figure 6A and assign it to our agent as follows:

```{r set_cues_6a}

cue_list_6a <- list(
  one = data.frame(
    onset = 60,
    offset = 80,
    magnitude = 1,
    trial = 1:500),
  two = data.frame(
    # Generate 500 random values for the second cue onset between 69 and 71
    onset = 68 + round(runif(500, 1,3)),
    offset = 80,
    magnitude = 1,
    trial = 1:500
  )
)

agent_6a %>%
  rl_set_cues(cue_list_6a) %>%
  print()

```

Again, by printing it, we can see that the cue structure has been set. With both our rewards and cues defined, we're ready to simulate the agent!

```{r simulate_agent_6a}
agent_6a %>%
  rl_simulate_agent() %>%
  print()
```

This time, when we print the agent, we can see that simulations have been complete! We found this helpful in tracking which agents need (or do not need) to be simulated. Now, the fun part is to use the simulation data and regenerate Figure 6A from Montague, Dayan, and Sejnowski (1996). To do that, we need to access the prediction error data. This can be gathered as a matrix directly from the agent object:[^1]

[^1]: If you prefer a 'tidier' version of this data (helfpul for plotting with [ggplot2](https://ggplot2.tidyverse.org/)) you can get that with `rl_get_pe_data()`. By default, this function adds a trial zero with zero prediction errors. This is to help with visualizing the RPEs over time. To change that, simply set `add_trial_zero = FALSE`.

```{r access-rpe-6a, eval = FALSE}
agent_6a$RPE
```

For visualizing a 3D surface plot as in the paper, we want to focus on episodes 40 to 90. Using [plotly](https://plotly.com/r/getting-started/), we can get a nice, interactive visualization of how the prediction errors evolve over time:

```{r plot-rpe-6a}

agent_6a$RPE[40:90,] %>%
  plotly::plot_ly(z = .) %>%
  plotly::add_surface() %>%
  plotly::layout(scene = list(xaxis = list(title = "Trial"),
                              yaxis = list(title = "Episode"),
                              zaxis = list(title = "RPE")),
                 title = "Reward Prediction Errors (RPE) over time")

```

