---
title: "K-Armed Bandit"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{k-armed-bandit}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
library(magrittr)
```

```{r setup}
library(rlsims)
library(ggplot2)
```

# Introduction to $K$-Armed Bandits

Multi-armed Bandits, also called $K$-armed Bandits describe a type of reinforcement learning problem whereby an agent selects one of *K* possible actions (arms) over a specified number of trials. After each selection, the agent experiences a reinforcement which influences the agent's action on the next trial. There are many real-world applications of multi-armed bandits. Consider a developer who wants to try different application icons and find which one is most appealing to customers and maximizes the number of downloads. They might use a multi-armed bandit to learn which $k$-icon maximizes positive reinforcements (number of downloads). 

Similar ideas are explained in greater detail in Alksandrs Slivkins textbook, [Introduction to Multi-Armed Bandits](https://arxiv.org/pdf/1904.07272.pdf). In the introduction, Slivkins describes the key points of bandit algorithms, highlighting the tradeoff between "exploration and exploitation: making optimal near-term decisions based on the available information." Exploration involves selecting new arms; exploitation involves selecting the arm that previously led to the best reinforcement.

This tradeoff is often formalized with a decision-making policy, describes how an agent makes a choice. In this package, we have implemented three widely-used policies: greedy, epsilon-greedy, and softmax. 

* An agent with a 'greedy' policy will always select the arm with the highest expected value. If, as in the first trial, all arms have the same expected value because they have not been sampled, the agent will choose randomly. 
* An agent with an 'epsilon-greedy' policy has a parameter `epsilon`, which describes the propensity to explore the action (arm) space. The higher the epsilon, the more likely the agent is to select a random action; the lower epsilon, the more likely the agent is to select the exploitative action (one with highest expected value).
* An agent with a 'softmax' policy has a parameter `tau`, which describes the propensity to explore the action space. The higher the tau (temperature), the more random the actions; the lower the tau (temperature), the more exploitative the actions (e.g., lower temperature increases the probability of taking the action with the highest expected value).

# Examples

## Case Study: Dissociable Effects of Dopamine and Serotonin on Reversal Learning

[den Ouden, Daw, and colleagues (2013)](https://doi.org/10.1016/j.neuron.2013.08.030) studied how polymorphisms in genes encoding dopaminergic and serotonin transporters affected behavioral adaptations in a two-Armed Bandit 'reversal learning' task. They had 810 participants complete an 80-trial experiment where individuals selected between a yellow and a blue pattern to maximize a positive reinforcement (reward; green happy emoticon) and minimize a negative reinforcement (punishment; red sad/angry emoticon). For the first forty trials, one pattern yielded a reward with 70% probability and a punishment with 30% probability; the other pattern yielded a punishment with 70% probability and a reward with 30% probability. For the second forty trials, the probabilistic reward and punishment structure for each stimulus switched. 

Participants were instructed to "Choose the color which tends to be correct more often. You have to find out by trial and error which color that is. On certain moments the rule can change, i.e. the other color is now correct more often. Then, switch your response to that color. This can happen one or more times during the task."[^1]

[^1]: From the supplemental information, accessible [here](https://www.cell.com/cms/10.1016/j.neuron.2013.08.030/attachment/0ff21f8c-619e-439d-b196-84bee8de7e97/mmc1.pdf).

Using rlsims, we can simulate choice behavior on this task. The authors compared two reinforcement learning algorithms, experience-weighted attraction (EWA; [Camerer and Ho, 2003](https://onlinelibrary.wiley.com/doi/abs/10.1111/1468-0262.00054)) and a modified Rescorla Wagner ([Frank et al., 2007](https://pubmed.ncbi.nlm.nih.gov/17913879/)), each with a softmax action-selection policy. Here, we demonstrate how to simulate a Q-Learning algorithm given the task constraints:[^2]

[^2]: As of January 4, 2022 the 'kArmed Bandit' only supports a [Q-Learning algorithm](https://en.wikipedia.org/wiki/Q-learning), which is what we describe here. This vignette will be updated in the future as additional algorithm types are supported.

### Define Agent

As with other Reinforcement Learning simulations using rlsims, we begin by creating an RL Agent with the specified model type (`model_type = "kArmedBandit"`). `model_id` is a (required) description to help distinguish multiple agents simulated in the same R session. From the experimental design, we know the task has 80 trials (`num_trials = 80`) and there are two arms (possible actions, `num_arms = 2`). We also need to specify the number of episodes per trial. In the simplest implementation, $k$-armed bandits repeatedly (i) select an action and (ii) experiences a reinforcement. Therefore, we must have at least two episodes. In the first, the agent selects an arm (performs an action); in the second, the agent receives feedback. Here, we will also add an additional episode at the beginning to represent the stimulus presentation. The algorithmic implementation we use here requires that the reinforcement happen before the terminal episode in a trial.[^3] That is,  `num_episodes = 3`, `action_episode = 2`, and `reinforcement_episode = 3`. For this vignette, we will arbitrarily define the temporal discounting factor, $0 \leq \gamma \leq 1$ as 1 and define a learning rate, $\alpha$ of 0.6.

[^3]: Take a look at the `simulate_agent()` method of the kArmedBandit agent we define (e.g., `denOudenAgent2013$simulate_agent`).

```{r new_agent-denOuden}

denOudenAgent2013 <- rl_new_agent(model_type = "kArmedBandit", 
                          model_id = "denOuden2013 2-Armed Bandit Example", 
                          num_trials = 80,
                          num_episodes = 4,
                          num_arms = 2, 
                          action_episode = 2,
                          reinforcement_episode = 3,
                          gamma = 1,
                          alpha = 0.6)
```

As with other RL objects, this one has special printing properties describing meta information:

```{r print-denOuden}
denOudenAgent2013
```

### Define Arm Structure

In order to simulate this agent, we need to define the arm structure and action-selection policy. In favoring explicit specifications (and to harmonize this package's API as much as possible), the arms are defined as a list of data frames -- similar to a 'tdrlConditioning' agent's reinforcement structure. To define an arm, you must specify the `probability` of receiving a reinforcement of `magnitude` or the `alternative` magnitude (received with 1 - probability) by selecting an arm on a given `trial`. 

For the task described, the 'blue' arm yields a positive reinforcement of `magnitude = 1` for the first forty trials with 70% probability `probability = rep(0.7, 40)` or no reinforcement (`alternative = 0`); on the second forty trials, selecting the 'blue' arm yields a positive reinforcement with 30% probability `probability = rep(0.3, 40)`. The 'yellow' arm does the opposite. Combining these definitions, we can set the arm structure for this agent as shown below. For more documentation on these requirements, call `?rl_arms_define` in the console.

```{r defineArms-denOuden}
denOudenArms <- list(
  blue = data.frame(
    probability = c(rep(0.7, 40), rep(0.3, 40)),
    magnitude = 1,
    alternative = 0,
    trial = 1:80
  ),
  blue = data.frame(
    probability = c(rep(0.3, 40), rep(0.4, 40)),
    magnitude = 1,
    alternative = 0,
    trial = 1:80
  )
)

denOudenAgent2013 %>%
  rl_set_arms(denOudenArms)
```

### Define Action-Selection Policy

The authors used a softmax decision policy and estimated an inverse temperature, $\beta$ from their behavioral data. That seems like a good starting point for our simulation, so we can use the function `rl_set_policy` to define how our agent will sample the action space. It accepts the name (`policy = "softmax"`) and, if applicable, the modulating parameters. In this package, we implement the softmax policy using the temperature parameter, $\tau$, to describe an agent's propensity to explore the action space. The higher the temperature, the more random the actions; the lower the temperature, the more exploitative the actions (e.g., lower temperature increases the probability of taking the action with the highest expected value).

den Ouden and colleagues estimated a median $\beta$ of $\approx 4.5$. By taking its inverse, we can set our action-selection policy as follows:

```{r set_policy-denOuden}
# They used inverse temperature, so we're converting that to temperature
denOudenAgent2013 %>%
  rl_set_policy(policy = "softmax", tau = 1/4.5)
```

### Simulate and Explore Results

Now it's time to simulate our agent and explore our results. As with other RL Agents, we use `rl_simulate_agent()`:

```{r simulate-denOuden}
denOudenAgent2013 %>%
  rl_simulate_agent()
```

To analyze the simulation data, we can use `rlsims::rl_get_pe_data()` and `rlsims::rl_get_learned_values()` to access the prediction error and estimated values of a given action/episode (depending on the algorithm the learned values definitions can vary). By default, these functions impute a 'trial zero' for plotting purposes, but that can be controlled with the arguments `add_trial_zero` and `trial_zero_value`. Below are the results of the simulation (plotting code removed for clarity but is available on GitHub!).

```{r denOuden-pe-plot, echo = FALSE}
denOudenAgent2013 %>%
  rl_get_pe_data() %>%
  ggplot(aes(x = trial, y = value, group = arm, color = as.factor(arm))) +
    stat_summary(geom = "line", fun = median, size = 1.5) +
    stat_summary(geom = "point", fun = median, size = 1.5, 
                 shape = 21, alpha = 0.7,
                 aes(fill = as.factor(arm))) +
    theme_classic() +
    scale_x_continuous(breaks = seq(0, 80, by = 10)) +
    theme(axis.title.y = element_text(size = 14),
          legend.position = "none",
          panel.border = element_rect(color = "black", fill = NA, size = 1)) +
    scale_color_manual(values = c("blue", "gold", "#4588bf",
                                  "#f99c03", "#8a70d3", "#838385")) +
  scale_fill_manual(values = c("skyblue", "cornsilk")) +
    labs(title = "Prediction Errors per Arm, per Episode",
         y = "Estimated Q Value", x = "Trial Number") +
  facet_wrap(~episode, 
             labeller = as_labeller(
               c(`1` = "Episode 1: Option Presentation",
                 `2` = "Episode 2: Action Selection",
                 `3` = "Episode 3: Reinforcement Delivery",
                 `4` = "Episode 4: Terminal Episode")
             )) +
  theme(strip.background = element_blank(),
        strip.text = element_text(size = 10, face = "bold"),
        plot.title = element_text(hjust = 0.5, size = 15))

```

```{r denOuden-learned-values-plot, echo = FALSE}

denOudenAgent2013 %>%
  rl_get_learned_values() %>%
  ggplot(aes(x = trial, y = q_value, group = arm, color = as.factor(arm))) +
    stat_summary(geom = "line", fun = median, size = 1.5) +
    stat_summary(geom = "point", fun = median, size = 1.5, 
                 shape = 21, alpha = 0.7,
                 aes(fill = as.factor(arm))) +
    theme_classic() +
    scale_x_continuous(breaks = seq(0, 80, by = 10)) +
    theme(axis.title.y = element_text(size = 14),
          legend.position = "none",
          panel.border = element_rect(color = "black", fill = NA, size = 1)) +
    scale_color_manual(values = c("blue", "gold", "#4588bf",
                                  "#f99c03", "#8a70d3", "#838385")) +
  scale_fill_manual(values = c("skyblue", "cornsilk")) +
    labs(title = "Q-Value Estimates per Arm, per Episode",
         y = "Estimated Q Value", x = "Trial Number") +
  facet_wrap(~episode, 
             labeller = as_labeller(
               c(`1` = "Episode 1: Option Presentation",
                 `2` = "Episode 2: Action Selection",
                 `3` = "Episode 3: Reinforcement Delivery",
                 `4` = "Episode 4: Terminal Episode")
             )) +
    theme(strip.background = element_blank(),
        strip.text = element_text(size = 10, face = "bold"),
        plot.title = element_text(hjust = 0.5, size = 15))

```

It looks like the agent did learn the blue arm was better chosen during the first half. Although they it seems the agent began learning the yellow arm was more beneficial in the latter half, it was not sampled nearly as much (`denOuden2013$actions` will show the agent's choices). This behavior can be modified with a higher temperature in the softmax decision policy, but we leave that experiment to you! 

With this case study, we've seen how to recreate bandit problems commonly used in neuroscientific investigations. An extension of this vignette, if you would like to try on your own, is to simulate multiple agents with similar -- but not identical -- learning rates ($\alpha$) and temperatures ($\tau$). Those simulations can be combined and analyzed to illustrate how certain behaviors might look on a population level and help inform experimental designs.[^4]

[^4]: If you notice in our plots, there are jerky movements which could be smoothed out by taking the median values across all simulated agents, for example.

