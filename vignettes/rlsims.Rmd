---
title: "Learn rlsims"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Learn rlsims }
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
library(plotly)
library(rlsims)
```

# Overview

> We hope to update this page with a brief introduction in the future. For now, we recommend you check out [this book by Sutton and Barto](http://incompleteideas.net/book/bookdraft2017nov5.pdf). 

Reinforcement learning is a computational paradigm describing how an agent learns through interacting with its environment in order to maximize positive reinforcements. 

## Example

To simulate an RL agent on a specific task/environment, you need to create a specific agent. Supported ones can be accessed by calling `rlsims::available_agents()`. For this demo, we'll walk through how to create an temporal-difference reinforcement learning (TDRL) agent to explore the concept of back-propagation, classical conditioning, whereby a positive prediction error signal, in response to an unexpected positive reinforcement, shifts from the time the reward is delivered to the earliest environmental cue that predicts it. Consider the following description:

> Over 100 trials, each with ten distinct timesteps or 'episodes', we presented a stimulus on the third episode which was always followed by a reward at the eighth episode.

We can model this with TDRL by creating an RL Agent of type 'tdrlConditioning'. First, we specify `model_type = "tdrlConditioning"` to ensure all methods are applied for the desired algorithm. The argument `model_id` is simply a description that can be useful for distinguishing between multiple agents simulated in the same R session. As described above, we are simulating over 100 trials, each of which has ten episodes. We're arbitrarily setting the temporal discounting factor, $0 \leq \gamma \leq 1$, to simplify the algorithm (`gamma = 1`). Similarly, we'll set learning rate, $\alpha$, as 0.3 (`alpha = 0.05`).
 
```{r tdrlCond-define}

tdrlCond <- rl_new_agent(model_type = "tdrlConditioning",
                         model_id = "Classical Conditioning via TDRL",
                         num_trials = 100, 
                         num_episodes = 10, 
                         gamma = 1, 
                         alpha = 0.3)
```

RL Agent objects have special printing properties, describing meta information for an agent:

```{r tdrlCond-show-print-method}
tdrlCond
```

Note that, although we've defined the number of trials and episodes, we haven't specified the timing of the cues or the reward. To do that, we will use the functions `rl_set_reinforcements()` and `rl_set_cues()`. Both functions take a list with a `data.frame` that has columns 'onset', 'offset', 'magnitude', and 'trial'. 'onset' describes the episode number the reinforcement and cue first appear, and the offset specifies the episode number their presentation should terminate. 'magnitude' dictates the saliency and value of the cue and reinforcement, respectively. 'trial' defines on which trials cues and reinforcements defined in the onset/offset/magnitude columns occur.

To begin, we'll specify the cue structure where it is present (`magnitude = 1`) at episode three (`onset = 3`) on all one-hundred trials (`trial = 1:100`). The way we chose to implement the TDRL algorithm, the cue must "remain" until the reward is delivered, which we specified as the eighth episode (`offset = 8`).[^1] We construct these parameters in a data frame and wrap it in a list. Doing so allows us to generalize the TDRL algorithm for environments with multiple cues. For this example, the cue structure looks like this:

[^1]: We plan to discuss the reasoning behind this in more detail in the future. For now, note that the cue's offset must equal the reward's onset.

```{r tdrlCond-specify_cues}

cues_tdrlCond <- list(
  one = data.frame(
    onset = 3,
    offset = 8,
    magnitude = 1,
    trial = 1:100
  )
)

```

We can set the cue list as follows:

```{r tdrlCond-set_cues}
tdrlCond %>%
  rl_set_cues(cue_input = cues_tdrlCond) %>%
  print()
```

By printing it, we can see that the cue structure has been registered. We'll now do the same for the reinforcement structure. A reinforcement of `magnitude = 1`, a positive reinforcement, is delivered only at episode eight (`onset = 8` and `offset = 8`) for all one-hundred trials (`trial = 1:100`).

```{r tdrlCond-specify-rewards}

reinforcements_tdrlCond <- list(
    data.frame(
      onset = 8,
      offset = 8,
      magnitude = 1,
      trial = 1:100
    )
  )

```

We can set the reinforcements, and see how the reinforcement structure has been updated, as follows:

```{r }
tdrlCond %>%
  rl_set_reinforcements(reinforcement_input = reinforcements_tdrlCond) %>%
  print()
```

With both our rewards and cues defined, we're ready to simulate the agent!

```{r simulate_agent_6a}
tdrlCond %>%
  rl_simulate_agent() %>%
  print()
```

When printing the agent this time, we can see that simulations have been complete![^2] Now that it's simulated, we can see how back-propagation occurs. To do that, we'll use `rlsims::rl_get_pe_data()` to access the prediction error data from the agent. For plotting purposes, we'll also specify `add_trial_zero = TRUE` and `trial_zero_value = 0`. When the reinforcement is first (and unexpectedly) delivered, our agent generates a positive reward prediction error (RPE) at episode eight.

[^2]: We found this feature helpful in tracking which agents need (or do not need) to be simulated. 

```{r tdrlCond-get_prediction_error}

pe_data_tdrlCond <- tdrlCond %>%
  rl_get_pe_data(add_trial_zero = TRUE, trial_zero_value = 0)

# Get the prediction error data from the firs trial (all episodes). This corresponds to rows 11:20, since we added a 'zero' trial for plotting.
pe_data_tdrlCond[11:20,]

```

Over time, the RPE at episode eight will diminish, and we'll see it occur at episode three (when the cue appears). Below is an interactive plot made with [plotly](https://plotly.com/r/getting-started/), so we visualize how the prediction errors back propagate over time (click the "Play" button!). For clarity, we're excluding the code to generate it here, but you can find it [on GitHub](https://github.com/jdtrat/rlsims/blob/main/vignettes/rlsims.Rmd) if you'd like!

```{r tdrlCond-pe-plot, echo = FALSE}

pe_data_tdrlCond %>%
  plot_ly(
    x = ~episode,
    y = ~value,
    frame = ~trial,
    type = 'scatter',
    mode = 'lines',
    showlegend = FALSE
  ) %>%
  layout(yaxis = list(title = "Reward Prediction Error",
                      range = c(0,1)),
         xaxis = list(title = "Episode",
                      tickvals = list(1,2,3,4,5,6,7,8,9,10))) %>%
  add_annotations(text = "Reward Prediction Error Over Time", 
                  x = 0.5,
                  y = 1,
                  yref = "paper",
                  xref = "paper",
                  yanchor = "bottom",
                  showarrow = FALSE,
                  font = list(size = 15)) %>%
  animation_button(x = 0, y = -0.2, xanchor = "left", yanchor = "bottom") %>%
  animation_slider(currentvalue = list(prefix = "TRIAL: ", font = list(color = "black"))) %>%
  animation_opts(
    frame = 100, 
    easing = "elastic"
  )
  
```
